{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = []\n",
    "annotated = []\n",
    "words_queue = []\n",
    "files = set()\n",
    "with zipfile.ZipFile('MACCROBAT2018.zip', 'r') as z:\n",
    "    for filename in z.namelist():\n",
    "        files.add(filename.split(\".\")[0])\n",
    "\n",
    "    for filename in files:\n",
    "        with z.open(filename + \".txt\") as file:\n",
    "            for line in file:\n",
    "                sentence = line.decode('utf-8').strip()\n",
    "                if sentence:\n",
    "                    input_text.append(sentence)\n",
    "\n",
    "with zipfile.ZipFile('MACCROBAT2020.zip', 'r') as z:\n",
    "    for filename in z.namelist():\n",
    "        files.add(filename.split(\".\")[0])\n",
    "\n",
    "    for filename in files:\n",
    "        with z.open(filename + \".txt\") as file:\n",
    "            for line in file:\n",
    "                sentence = line.decode('utf-8').strip()\n",
    "                if sentence:\n",
    "                    input_text.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def generate_detailed_tags(sentences):\n",
    "    detailed_tags = []\n",
    "\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if index % 1000 == 0:\n",
    "            print(index)\n",
    "\n",
    "        # Process the sentence with spaCy\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        # Initialize BIO tags as \"O\" for all tokens\n",
    "        bio_tags = [\"O\"] * len(doc)\n",
    "\n",
    "        # Assign B- and I- tags based on entities\n",
    "        for ent in doc.ents:\n",
    "            start = ent.start\n",
    "            end = ent.end\n",
    "\n",
    "            # First token in the entity gets B- prefix\n",
    "            bio_tags[start] = f\"B-{ent.label_}\"\n",
    "\n",
    "            # Subsequent tokens in the entity get I- prefix\n",
    "            for i in range(start + 1, end):\n",
    "                bio_tags[i] = f\"I-{ent.label_}\"\n",
    "\n",
    "        # Collect tokens, POS tags, and BIO tags\n",
    "        tokens = [token.text for token in doc]\n",
    "        pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "        # Add to the output\n",
    "        detailed_tags.append({\n",
    "            \"sentence\": sentence,\n",
    "            \"tokens\": tokens,\n",
    "            \"pos_tags\": pos_tags,\n",
    "            \"ner_tags\": bio_tags\n",
    "        })\n",
    "\n",
    "    return detailed_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "# Generate detailed tags\n",
    "detailed_tags_output = generate_detailed_tags(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: A 46-year-old Caucasian woman with type 2 diabetes mellitus and bipolar disorder presented to our emergency department with vague abdominal symptoms and vomiting.\n",
      "Tokens: ['A', '46', '-', 'year', '-', 'old', 'Caucasian', 'woman', 'with', 'type', '2', 'diabetes', 'mellitus', 'and', 'bipolar', 'disorder', 'presented', 'to', 'our', 'emergency', 'department', 'with', 'vague', 'abdominal', 'symptoms', 'and', 'vomiting', '.']\n",
      "POS Tags: ['DET', 'NUM', 'PUNCT', 'NOUN', 'PUNCT', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'NUM', 'NOUN', 'ADJ', 'CCONJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'PRON', 'NOUN', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'CCONJ', 'NOUN', 'PUNCT']\n",
      "NER Tags: ['O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'B-NORP', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Sentence: Her pertinent history includes left below knee amputation and right toes amputation for complications secondary to diabetic neuropathy.\n",
      "Tokens: ['Her', 'pertinent', 'history', 'includes', 'left', 'below', 'knee', 'amputation', 'and', 'right', 'toes', 'amputation', 'for', 'complications', 'secondary', 'to', 'diabetic', 'neuropathy', '.']\n",
      "POS Tags: ['PRON', 'ADJ', 'NOUN', 'VERB', 'VERB', 'ADP', 'NOUN', 'NOUN', 'CCONJ', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'PART', 'VERB', 'ADJ', 'PUNCT']\n",
      "NER Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Sentence: At the time of admission, she was undergoing care for an infected diabetic ulcer of her right foot.\n",
      "Tokens: ['At', 'the', 'time', 'of', 'admission', ',', 'she', 'was', 'undergoing', 'care', 'for', 'an', 'infected', 'diabetic', 'ulcer', 'of', 'her', 'right', 'foot', '.']\n",
      "POS Tags: ['ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'VERB', 'NOUN', 'ADP', 'DET', 'VERB', 'ADJ', 'NOUN', 'ADP', 'PRON', 'ADJ', 'NOUN', 'PUNCT']\n",
      "NER Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Sentence: Of note, she did not have a history of CAPD or a history of renal disease: creatinine 1.23 mg/dL, blood urea nitrogen (BUN) 16 mg/dL.\n",
      "Tokens: ['Of', 'note', ',', 'she', 'did', 'not', 'have', 'a', 'history', 'of', 'CAPD', 'or', 'a', 'history', 'of', 'renal', 'disease', ':', 'creatinine', '1.23', 'mg', '/', 'dL', ',', 'blood', 'urea', 'nitrogen', '(', 'BUN', ')', '16', 'mg', '/', 'dL.']\n",
      "POS Tags: ['ADP', 'VERB', 'PUNCT', 'PRON', 'AUX', 'PART', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'PUNCT', 'NOUN', 'NUM', 'PROPN', 'SYM', 'PROPN', 'PUNCT', 'NOUN', 'NOUN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'NUM', 'ADP', 'SYM', 'PROPN']\n",
      "NER Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-QUANTITY', 'I-QUANTITY', 'I-QUANTITY', 'I-QUANTITY']\n",
      "\n",
      "Sentence: Her blood glucose levels were poorly controlled via subcutaneous insulin injection; she reported a range of 400 to 500 mg/dL at home (due to poor drug compliance).\n",
      "Tokens: ['Her', 'blood', 'glucose', 'levels', 'were', 'poorly', 'controlled', 'via', 'subcutaneous', 'insulin', 'injection', ';', 'she', 'reported', 'a', 'range', 'of', '400', 'to', '500', 'mg', '/', 'dL', 'at', 'home', '(', 'due', 'to', 'poor', 'drug', 'compliance', ')', '.']\n",
      "POS Tags: ['PRON', 'NOUN', 'NOUN', 'NOUN', 'AUX', 'ADV', 'VERB', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'NUM', 'PART', 'NUM', 'NOUN', 'SYM', 'NUM', 'ADP', 'NOUN', 'PUNCT', 'ADJ', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'PUNCT']\n",
      "NER Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print output\n",
    "for entry in detailed_tags_output[:5]:\n",
    "    print(f\"Sentence: {entry['sentence']}\")\n",
    "    print(f\"Tokens: {entry['tokens']}\")\n",
    "    print(f\"POS Tags: {entry['pos_tags']}\")\n",
    "    print(f\"NER Tags: {entry['ner_tags']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to 'tagged_medical_sentences.json'\n"
     ]
    }
   ],
   "source": [
    "# Save to a JSON file\n",
    "with open(\"tagged_medical_sentences.json\", \"w\") as f:\n",
    "    json.dump(detailed_tags_output, f, indent=4)\n",
    "\n",
    "print(\"Output saved to 'tagged_medical_sentences.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    data.dropna(inplace=True)\n",
    "    print(\"Number of rows : \",data.shape[0],\" and the number of columns : \",data.shape[1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip general.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows :  47959  and the number of columns :  4\n"
     ]
    }
   ],
   "source": [
    "data = load_data(\"ner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n"
     ]
    }
   ],
   "source": [
    "# Generate detailed tags\n",
    "detailed_tags_output = generate_detailed_tags(data[\"Sentence\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "Tokens: ['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']\n",
      "POS Tags: ['NOUN', 'ADP', 'NOUN', 'AUX', 'VERB', 'ADP', 'PROPN', 'PART', 'VERB', 'DET', 'NOUN', 'ADP', 'PROPN', 'CCONJ', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "NER Tags: ['B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Sentence: Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"\n",
      "Tokens: ['Families', 'of', 'soldiers', 'killed', 'in', 'the', 'conflict', 'joined', 'the', 'protesters', 'who', 'carried', 'banners', 'with', 'such', 'slogans', 'as', '\"', 'Bush', 'Number', 'One', 'Terrorist', '\"', 'and', '\"', 'Stop', 'the', 'Bombings', '.', '\"']\n",
      "POS Tags: ['NOUN', 'ADP', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'PUNCT', 'PROPN', 'PROPN', 'NUM', 'PROPN', 'PUNCT', 'CCONJ', 'PUNCT', 'VERB', 'DET', 'PROPN', 'PUNCT', 'PUNCT']\n",
      "NER Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'O', 'O']\n",
      "\n",
      "Sentence: They marched from the Houses of Parliament to a rally in Hyde Park .\n",
      "Tokens: ['They', 'marched', 'from', 'the', 'Houses', 'of', 'Parliament', 'to', 'a', 'rally', 'in', 'Hyde', 'Park', '.']\n",
      "POS Tags: ['PRON', 'VERB', 'ADP', 'DET', 'PROPN', 'ADP', 'PROPN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'PUNCT']\n",
      "NER Tags: ['O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O']\n",
      "\n",
      "Sentence: Police put the number of marchers at 10,000 while organizers claimed it was 1,00,000 .\n",
      "Tokens: ['Police', 'put', 'the', 'number', 'of', 'marchers', 'at', '10,000', 'while', 'organizers', 'claimed', 'it', 'was', '1,00,000', '.']\n",
      "POS Tags: ['NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NUM', 'SCONJ', 'NOUN', 'VERB', 'PRON', 'AUX', 'NUM', 'PUNCT']\n",
      "NER Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O']\n",
      "\n",
      "Sentence: The protest comes on the eve of the annual conference of Britain 's ruling Labor Party in the southern English seaside resort of Brighton .\n",
      "Tokens: ['The', 'protest', 'comes', 'on', 'the', 'eve', 'of', 'the', 'annual', 'conference', 'of', 'Britain', \"'s\", 'ruling', 'Labor', 'Party', 'in', 'the', 'southern', 'English', 'seaside', 'resort', 'of', 'Brighton', '.']\n",
      "POS Tags: ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'PROPN', 'PART', 'VERB', 'PROPN', 'PROPN', 'ADP', 'DET', 'ADJ', 'PROPN', 'NOUN', 'NOUN', 'ADP', 'PROPN', 'PUNCT']\n",
      "NER Tags: ['O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'B-DATE', 'O', 'O', 'B-GPE', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-LANGUAGE', 'O', 'O', 'O', 'B-GPE', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print output\n",
    "for entry in detailed_tags_output[:5]:\n",
    "    print(f\"Sentence: {entry['sentence']}\")\n",
    "    print(f\"Tokens: {entry['tokens']}\")\n",
    "    print(f\"POS Tags: {entry['pos_tags']}\")\n",
    "    print(f\"NER Tags: {entry['ner_tags']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to 'tagged_general_sentences.json'\n"
     ]
    }
   ],
   "source": [
    "# Save to a JSON file\n",
    "with open(\"tagged_general_sentences.json\", \"w\") as f:\n",
    "    json.dump(detailed_tags_output, f, indent=4)\n",
    "\n",
    "print(\"Output saved to 'tagged_general_sentences.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
