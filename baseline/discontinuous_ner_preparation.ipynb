{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discontinuous NER Data Preparation\n",
    "\n",
    "This notebook handles the preparation of discontinuous NER datasets with different positive/negative ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/gareginmazmanyan/Documents/UOFA/CSC580/FinalProject/NER\n",
      "Data directory: /Users/gareginmazmanyan/Documents/UOFA/CSC580/FinalProject/NER/data/discontinuous\n"
     ]
    }
   ],
   "source": [
    "PROJ_ROOT = Path.cwd().parent  \n",
    "DATA_DIR = PROJ_ROOT / 'data' / 'discontinuous'\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Project root: {PROJ_ROOT}')\n",
    "print(f'Data directory: {DATA_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47959 sentences from existing data\n"
     ]
    }
   ],
   "source": [
    "# Load existing data\n",
    "def load_existing_data():\n",
    "    data_path = PROJ_ROOT / 'data' / 'tagged_general_sentences.json'\n",
    "    with open(data_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "general_data = load_existing_data()\n",
    "print(f'Loaded {len(general_data)} sentences from existing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscontinuousNERHandler:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.data_dir = DATA_DIR\n",
    "    \n",
    "    def create_discontinuous_example(self, text: str) -> Tuple[List[Tuple[str, str]], List[str]]:\n",
    "        \"\"\"Create a discontinuous NER example from input text\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        words_with_pos = []\n",
    "        labels = []\n",
    "        \n",
    "        entities = [(e.text, e.label_, e.start_char, e.end_char) for e in doc.ents]\n",
    "        \n",
    "        for token in doc:\n",
    "            words_with_pos.append((token.text, token.pos_))\n",
    "            \n",
    "            # Default to non-entity\n",
    "            label = 'O'\n",
    "            \n",
    "            # Check if token is part of an entity\n",
    "            for ent_text, ent_label, start, end in entities:\n",
    "                if token.idx >= start and token.idx < end:\n",
    "                    # Create discontinuous pattern for longer entities\n",
    "                    if len(ent_text.split()) > 2 and random.random() < 0.4:\n",
    "                        if token.idx > start and token.idx < end - len(token.text):\n",
    "                            label = 'O' if random.random() < 0.5 else f'I-{ent_label}'\n",
    "                        else:\n",
    "                            label = f'B-{ent_label}' if token.idx == start else f'I-{ent_label}'\n",
    "                    else:\n",
    "                        label = f'B-{ent_label}' if token.idx == start else f'I-{ent_label}'\n",
    "                    break\n",
    "            \n",
    "            labels.append(label)\n",
    "        \n",
    "        return words_with_pos, labels\n",
    "    \n",
    "    def prepare_dataset(self, texts: List[str], ratio: float) -> Tuple[List, List]:\n",
    "        \"\"\"Prepare train and validation datasets with given positive ratio\"\"\"\n",
    "        processed_examples = []\n",
    "        \n",
    "        for text in texts:\n",
    "            words_pos, labels = self.create_discontinuous_example(text)\n",
    "            processed_examples.append((words_pos, labels))\n",
    "        \n",
    "        # Split into positive and negative examples\n",
    "        positive = [ex for ex in processed_examples if any(l != 'O' for l in ex[1])]\n",
    "        negative = [ex for ex in processed_examples if all(l == 'O' for l in ex[1])]\n",
    "        \n",
    "        # Calculate desired numbers\n",
    "        total = len(processed_examples)\n",
    "        num_positive = int(total * ratio)\n",
    "        num_negative = total - num_positive\n",
    "        \n",
    "        # Sample examples\n",
    "        sampled_positive = random.sample(positive, min(num_positive, len(positive)))\n",
    "        sampled_negative = random.sample(negative, min(num_negative, len(negative)))\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        dataset = sampled_positive + sampled_negative\n",
    "        random.shuffle(dataset)\n",
    "        \n",
    "        # Split into train/val\n",
    "        split_idx = int(len(dataset) * 0.8)\n",
    "        return dataset[:split_idx], dataset[split_idx:]\n",
    "    \n",
    "    def save_datasets(self, train_data: List, val_data: List, ratio: float):\n",
    "        \"\"\"Save datasets to files\"\"\"\n",
    "        # Save training data\n",
    "        train_path = self.data_dir / f'train_ratio_{ratio:.1f}.json'\n",
    "        with open(train_path, 'w') as f:\n",
    "            json.dump(train_data, f)\n",
    "        \n",
    "        # Save validation data\n",
    "        val_path = self.data_dir / f'val_ratio_{ratio:.1f}.json'\n",
    "        with open(val_path, 'w') as f:\n",
    "            json.dump(val_data, f)\n",
    "        \n",
    "        print(f'Saved datasets with ratio {ratio:.1f}:')\n",
    "        print(f'Training examples: {len(train_data)}')\n",
    "        print(f'Validation examples: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 sentences\n",
      "\n",
      "Example of processed data:\n",
      "\n",
      "Word-POS pairs: [('Thousands', 'NOUN'), ('of', 'ADP'), ('demonstrators', 'NOUN'), ('have', 'AUX'), ('marched', 'VERB')] ...\n",
      "NER tags: ['B-CARDINAL', 'O', 'O', 'O', 'O'] ...\n",
      "Saved datasets with ratio 0.2:\n",
      "Training examples: 237\n",
      "Validation examples: 60\n",
      "\n",
      "Created dataset with ratio 0.2\n",
      "Training examples: 237\n",
      "Validation examples: 60\n",
      "--------------------------------------------------\n",
      "Saved datasets with ratio 0.3:\n",
      "Training examples: 317\n",
      "Validation examples: 80\n",
      "\n",
      "Created dataset with ratio 0.3\n",
      "Training examples: 317\n",
      "Validation examples: 80\n",
      "--------------------------------------------------\n",
      "Saved datasets with ratio 0.4:\n",
      "Training examples: 397\n",
      "Validation examples: 100\n",
      "\n",
      "Created dataset with ratio 0.4\n",
      "Training examples: 397\n",
      "Validation examples: 100\n",
      "--------------------------------------------------\n",
      "Saved datasets with ratio 0.5:\n",
      "Training examples: 477\n",
      "Validation examples: 120\n",
      "\n",
      "Created dataset with ratio 0.5\n",
      "Training examples: 477\n",
      "Validation examples: 120\n",
      "--------------------------------------------------\n",
      "Saved datasets with ratio 0.6:\n",
      "Training examples: 557\n",
      "Validation examples: 140\n",
      "\n",
      "Created dataset with ratio 0.6\n",
      "Training examples: 557\n",
      "Validation examples: 140\n",
      "--------------------------------------------------\n",
      "Saved datasets with ratio 0.7:\n",
      "Training examples: 637\n",
      "Validation examples: 160\n",
      "\n",
      "Created dataset with ratio 0.7\n",
      "Training examples: 637\n",
      "Validation examples: 160\n",
      "--------------------------------------------------\n",
      "Saved datasets with ratio 0.8:\n",
      "Training examples: 717\n",
      "Validation examples: 180\n",
      "\n",
      "Created dataset with ratio 0.8\n",
      "Training examples: 717\n",
      "Validation examples: 180\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create datasets with different ratios\n",
    "handler = DiscontinuousNERHandler()\n",
    "ratios = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "processed_data = []\n",
    "for item in general_data[:1000]: \n",
    "    # Create word-POS pairs\n",
    "    word_pos_pairs = list(zip(item['tokens'], item['pos_tags']))\n",
    "    # Get the corresponding NER tags\n",
    "    ner_tags = item['ner_tags']\n",
    "    # Add to processed data\n",
    "    processed_data.append((word_pos_pairs, ner_tags))\n",
    "\n",
    "print(f\"Processed {len(processed_data)} sentences\")\n",
    "print(\"\\nExample of processed data:\")\n",
    "example = processed_data[0]\n",
    "print(\"\\nWord-POS pairs:\", example[0][:5], \"...\")  # Show first 5 pairs\n",
    "print(\"NER tags:\", example[1][:5], \"...\")  # Show first 5 tags\n",
    "\n",
    "for ratio in ratios:\n",
    "    train_data, val_data = handler.prepare_dataset([item['sentence'] for item in general_data[:1000]], ratio)\n",
    "    handler.save_datasets(train_data, val_data, ratio)\n",
    "    print(f\"\\nCreated dataset with ratio {ratio:.1f}\")\n",
    "    print(f\"Training examples: {len(train_data)}\")\n",
    "    print(f\"Validation examples: {len(val_data)}\")\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example from ratio 0.2 dataset:\n",
      "Text: He said no outlawed organization will be allowed to collect donations and action will be taken against those who preach hate in mosques .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'O']\n",
      "\n",
      "Example from ratio 0.3 dataset:\n",
      "Text: Officials say the aircraft was supporting a NATO mission in the country 's south , but there is no indication of enemy action causing the crash .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Example from ratio 0.4 dataset:\n",
      "Text: Japan 's Foreign Minister , Nobutaka Machimura , arrives Sunday in Beijing for talks with his Chinese counterpart , Li Zhaoxing , to discuss relations between the two countries .\n",
      "Labels: ['B-GPE', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'B-DATE', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O']\n",
      "\n",
      "Example from ratio 0.5 dataset:\n",
      "Text: But he says those who become corrupt will be punished without mercy .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Example from ratio 0.6 dataset:\n",
      "Text: One of the 2008 Olympic mascots is modeled on a panda called Jing Jing .\n",
      "Labels: ['B-CARDINAL', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O']\n",
      "\n",
      "Example from ratio 0.7 dataset:\n",
      "Text: There has been no independent verification of the air strikes .\n",
      "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Example from ratio 0.8 dataset:\n",
      "Text: Palestinians identified him as Shahdi Mohanna , the Islamic Jihad commander for the northern Gaza Strip .\n",
      "Labels: ['B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Verify the results\n",
    "def inspect_dataset(ratio: float):\n",
    "    train_path = DATA_DIR / f'train_ratio_{ratio:.1f}.json'\n",
    "    with open(train_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f'\\nExample from ratio {ratio:.1f} dataset:')\n",
    "    example = random.choice(data)\n",
    "    words = [word for word, _ in example[0]]\n",
    "    labels = example[1]\n",
    "    \n",
    "    print('Text:', ' '.join(words))\n",
    "    print('Labels:', labels)\n",
    "\n",
    "for ratio in ratios:\n",
    "    inspect_dataset(ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
